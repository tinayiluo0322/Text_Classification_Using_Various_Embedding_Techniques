{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Report\n",
    "\n",
    "### Overview \n",
    "Raw counts, TF-IDF, LSA, and Word2Vec are different methods for preparing text data for classification using Logistic Regression. The document classification task is to distinguish  between \"Sense and Sensibility\" by Jane Austen and \"Alice's Adventures in Wonderland\" by Lewis Carroll.\n",
    "\n",
    "### Performance\n",
    "1. **Raw Counts**:\n",
    "   - Training accuracy: 98.87%\n",
    "   - Testing accuracy: 96.87%\n",
    "\n",
    "2. **TF-IDF**:\n",
    "   - Training accuracy: 99.93%\n",
    "   - Testing accuracy: 97.16%\n",
    "\n",
    "3. **LSA**:\n",
    "   - Training accuracy: 96.77%\n",
    "   - Testing accuracy: 95.37%\n",
    "\n",
    "4. **Word2Vec**:\n",
    "   - Training accuracy: 93.70%\n",
    "   - Testing accuracy: 91.34%\n",
    "\n",
    "### Comparison\n",
    "\n",
    "1. **Raw Counts vs. TF-IDF**: The TF-IDF method slightly outperformed raw counts on the test data. This is expected because raw counts consider only the frequency of terms within a document without considering the context or the order of terms, while TF-IDF not only considers the frequency of a term in a document but also its uniqueness across all documents. This gives more weight to terms that are more distinguishing between the two authors, making the classification a bit more effective. However, it is worth noting that the training accuracy for TF-IDF is very high (almost 100%), which could suggest that the model might be overfitting to the training data.\n",
    "\n",
    "2. **Raw Counts and TF-IDF vs. LSA**: LSA showed a decrease in performance compared to raw counts and TF-IDF. LSA is a technique for reducing dimensionality that may lead to the loss of significant information, especially in sparse datasets where it might not perform optimally. While LSA can capture some semantic relationships between words, it may lose critical information for distinguishing between the two authors, leading to lower accuracy. Furthermore, the choice of 300 dimensions may not be precisely tuned for this particular dataset, potentially contributing to decreased accuracy.\n",
    "\n",
    "3. **Raw Counts, TF-IDF, and LSA vs. Word2Vec**: Word2Vec had the lowest performance of all methods. This might be because Word2Vec represents words in a continuous vector space where semantically similar words are mapped to nearby points. While this is effective for capturing semantic similarities, it might not be as effective for authorship attribution where specific word choices and stylistic features are more important. Additionally, the pre-trained Word2Vec model is based on Google News data and uses 300-dimensional vectors. This generic training corpus may not include the domain-specific lexicon required for this particular classification task. Moreover, if the fixed 300-dimensional setting is not optimal for this dataset at hand, it can further diminish accuracy. Additionally, the predetermined context window size used by the Google News model to capture word co-occurrences could result in suboptimal representations for this specific task, potentially contributing to the reduced accuracy observed.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "The accuracy metric indicates how well the model correctly classifies the given sentences. Among the methods, TF-IDF yielded the best training and testing accuracy of 97.16%, suggesting it effectively captured distinguishing features between the two authors by emphasizing the importance of specific terms while diminishing the weight of frequently occurring but less informative terms. Raw counts provided the second-best training and testing accuracy, which works well as a direct and strong baseline when the frequency of word occurrence itself is a good indicator of authorship. LSA has the second lowest training and testing accuracy. Its dimensionality reduction can lead to the loss of vital information, particularly when dealing with sparse datasets. Word2Vec underperformed with the lowest test accuracy of 91.34%. This might be because Word2Vec focuses on semantic meanings, which may not be as crucial for authorship attribution as specific stylistic choices. Moreover, Word2Vec's training on Google News articles could miss nuances pertinent to this task. The fixed size of the context window used in the pre-trained model might also be misaligned with the data, causing suboptimal word or phrase representations. Furthermore, the chosen dimensionality of 300 for both LSA and Word2Vec may not be optimal for this dataset. It is notable that for all methods, the training accuracy was higher than the testing accuracy. This discrepancy hints that the models might be fitting closely to the training data, with TF-IDF showing an almost perfect training accuracy of 99.93%, raising concerns about potential overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']']\n",
      "['CHAPTER', '1']\n",
      "['The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex', '.']\n",
      "['Their', 'estate', 'was', 'large', ',', 'and', 'their', 'residence', 'was', 'at', 'Norland', 'Park', ',', 'in', 'the', 'centre', 'of', 'their', 'property', ',', 'where', ',', 'for', 'many', 'generations', ',', 'they', 'had', 'lived', 'in', 'so', 'respectable', 'a', 'manner', 'as', 'to', 'engage', 'the', 'general', 'good', 'opinion', 'of', 'their', 'surrounding', 'acquaintance', '.']\n",
      "['The', 'late', 'owner', 'of', 'this', 'estate', 'was', 'a', 'single', 'man', ',', 'who', 'lived', 'to', 'a', 'very', 'advanced', 'age', ',', 'and', 'who', 'for', 'many', 'years', 'of', 'his', 'life', ',', 'had', 'a', 'constant', 'companion', 'and', 'housekeeper', 'in', 'his', 'sister', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/tinayiluo/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"gutenberg\")\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Load the sentences from \"Sense and Sensibility\"\n",
    "austen_raw = nltk.corpus.gutenberg.sents(\"austen-sense.txt\")\n",
    "\n",
    "for sentence in austen_raw[:5]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw counts (train): 0.9887267904509284\n",
      "raw_counts (test): 0.9686567164179104\n",
      "tfidf (train): 0.9993368700265252\n",
      "tfidf (test): 0.9716417910447761\n",
      "lsa (train): 0.9670092838196287\n",
      "lsa (test): 0.9522388059701492\n",
      "word2vec (train): 0.9370026525198939\n",
      "word2vec (test): 0.9134328358208955\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Compare token/document vectors for classification.\"\"\"\n",
    "import random\n",
    "from typing import List, Mapping, Optional, Sequence\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "FloatArray = NDArray[np.float64]\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "# print(api.info())  # show info about available models/datasets\n",
    "\n",
    "# Un-comment this to fix the random seed\n",
    "random.seed(31)\n",
    "\n",
    "austen = nltk.corpus.gutenberg.sents(\"austen-sense.txt\")\n",
    "carroll = nltk.corpus.gutenberg.sents(\"carroll-alice.txt\")\n",
    "vocabulary = sorted(\n",
    "    set(token for sentence in austen + carroll for token in sentence)\n",
    ") + [None]\n",
    "\n",
    "vocabulary_map = {token: idx for idx, token in enumerate(vocabulary)}\n",
    "\n",
    "\n",
    "def onehot(\n",
    "    vocabulary_map: Mapping[Optional[str], int], token: Optional[str]\n",
    ") -> FloatArray:\n",
    "    \"\"\"Generate the one-hot encoding for the provided token in the provided vocabulary.\"\"\"\n",
    "    embedding = np.zeros((len(vocabulary_map),))\n",
    "    idx = vocabulary_map.get(token, len(vocabulary_map) - 1)\n",
    "    embedding[idx] = 1\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def sum_token_embeddings(\n",
    "    token_embeddings: Sequence[FloatArray],\n",
    ") -> FloatArray:\n",
    "    \"\"\"Sum the token embeddings.\"\"\"\n",
    "    total: FloatArray = np.array(token_embeddings).sum(axis=0)\n",
    "    return total\n",
    "\n",
    "\n",
    "def split_train_test(\n",
    "    X: FloatArray, y: FloatArray, test_percent: float = 10\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Split data into training and testing sets.\"\"\"\n",
    "    N = len(y)\n",
    "    data_idx = list(range(N))\n",
    "    random.shuffle(data_idx)\n",
    "    break_idx = round(test_percent / 100 * N)\n",
    "    training_idx = data_idx[break_idx:]\n",
    "    testing_idx = data_idx[:break_idx]\n",
    "    X_train = X[training_idx, :]\n",
    "    y_train = y[training_idx]\n",
    "    X_test = X[testing_idx, :]\n",
    "    y_test = y[testing_idx]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def generate_data_token_counts(\n",
    "    h0_documents: list[list[str]], h1_documents: list[list[str]]\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with raw token counts.\"\"\"\n",
    "    X: FloatArray = np.array(\n",
    "        [\n",
    "            sum_token_embeddings([onehot(vocabulary_map, token) for token in sentence])\n",
    "            for sentence in h0_documents\n",
    "        ]\n",
    "        + [\n",
    "            sum_token_embeddings([onehot(vocabulary_map, token) for token in sentence])\n",
    "            for sentence in h1_documents\n",
    "        ]\n",
    "    )\n",
    "    y: FloatArray = np.array(\n",
    "        [0 for sentence in h0_documents] + [1 for sentence in h1_documents]\n",
    "    )\n",
    "    return split_train_test(X, y)\n",
    "\n",
    "\n",
    "def generate_data_tfidf(\n",
    "    h0_documents: list[list[str]], h1_documents: list[list[str]]\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with TF-IDF scaling.\"\"\"\n",
    "    X_train, y_train, X_test, y_test = generate_data_token_counts(\n",
    "        h0_documents, h1_documents\n",
    "    )\n",
    "    tfidf = TfidfTransformer(norm=None).fit(X_train)\n",
    "    X_train = tfidf.transform(X_train)\n",
    "    X_test = tfidf.transform(X_test)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def generate_data_lsa(\n",
    "    h0_documents: list[list[str]], h1_documents: list[list[str]]\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with LSA.\"\"\"\n",
    "    X_train, y_train, X_test, y_test = generate_data_token_counts(\n",
    "        h0_documents, h1_documents\n",
    "    )\n",
    "    lsa = TruncatedSVD(n_components=300).fit(X_train)\n",
    "    X_train = lsa.transform(X_train)\n",
    "    X_test = lsa.transform(X_test)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def generate_data_word2vec(\n",
    "    h0_documents: list[list[str]], h1_documents: list[list[str]]\n",
    ") -> tuple[FloatArray, FloatArray, FloatArray, FloatArray]:\n",
    "    \"\"\"Generate training and testing data with word2vec.\"\"\"\n",
    "    # Load pretrained word2vec model from gensim\n",
    "    model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "    def get_document_vector(sentence: list[str]) -> NDArray:\n",
    "        \"\"\"Return document vector by summing word vectors.\"\"\"\n",
    "        vectors = [model[word] for word in sentence if word in model.key_to_index]\n",
    "        if vectors:\n",
    "            return np.sum(vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(\n",
    "                300\n",
    "            )  # return zero vector if no word in the document has a pretrained vector\n",
    "\n",
    "    # Produce document vectors for each sentence\n",
    "    X = np.array(\n",
    "        [get_document_vector(sentence) for sentence in h0_documents + h1_documents]\n",
    "    )\n",
    "    y = np.array([0 for sentence in h0_documents] + [1 for sentence in h1_documents])\n",
    "    return split_train_test(X, y)\n",
    "\n",
    "\n",
    "def run_experiment() -> None:\n",
    "    \"\"\"Compare performance with different embeddiings.\"\"\"\n",
    "    X_train, y_train, X_test, y_test = generate_data_token_counts(austen, carroll)\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "    print(\"raw counts (train):\", clf.score(X_train, y_train))\n",
    "    print(\"raw_counts (test):\", clf.score(X_test, y_test))\n",
    "    X_train, y_train, X_test, y_test = generate_data_tfidf(austen, carroll)\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "    print(\"tfidf (train):\", clf.score(X_train, y_train))\n",
    "    print(\"tfidf (test):\", clf.score(X_test, y_test))\n",
    "    X_train, y_train, X_test, y_test = generate_data_lsa(austen, carroll)\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "    print(\"lsa (train):\", clf.score(X_train, y_train))\n",
    "    print(\"lsa (test):\", clf.score(X_test, y_test))\n",
    "    X_train, y_train, X_test, y_test = generate_data_word2vec(austen, carroll)\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "    print(\"word2vec (train):\", clf.score(X_train, y_train))\n",
    "    print(\"word2vec (test):\", clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
